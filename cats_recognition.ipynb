{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "from torch import amp\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, random_split, WeightedRandomSampler\n",
    "import numpy as np\n",
    "import onnx\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- CONFIG ----------------\n",
    "DATASET_DIR = Path(\"id_dataset\")\n",
    "IMG_SIZE = 224\n",
    "NUM_CLASSES = 2\n",
    "EPOCHS = 15\n",
    "FREEZE_EPOCHS = 5\n",
    "BATCH_SIZE = 8\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 1e-2\n",
    "VALID_SPLIT = 0.2\n",
    "SEED = 42\n",
    "\n",
    "DEVICE = (\n",
    "    \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cuda\" if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['devi', 'sati']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jc/0m_v96ps7h12t3pj03psgnwr0000gn/T/ipykernel_8253/4055074135.py:103: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  scaler = amp.GradScaler()\n",
      "Epoch 1 [TRAIN]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [00:20<00:00,  8.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/15\n",
      "Train Acc: 79.44%\n",
      "Val   Acc: 91.01%\n",
      "LR: 9.89e-05\n",
      "\n",
      "âœ” Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [TRAIN]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [00:19<00:00,  8.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/15\n",
      "Train Acc: 89.07%\n",
      "Val   Acc: 93.33%\n",
      "LR: 9.57e-05\n",
      "\n",
      "âœ” Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [TRAIN]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [00:20<00:00,  8.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/15\n",
      "Train Acc: 90.66%\n",
      "Val   Acc: 95.94%\n",
      "LR: 9.05e-05\n",
      "\n",
      "âœ” Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [TRAIN]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [00:19<00:00,  8.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/15\n",
      "Train Acc: 93.12%\n",
      "Val   Acc: 93.33%\n",
      "LR: 8.35e-05\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [TRAIN]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [00:20<00:00,  8.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/15\n",
      "Train Acc: 93.99%\n",
      "Val   Acc: 96.23%\n",
      "LR: 7.50e-05\n",
      "\n",
      "âœ” Saved best model\n",
      "ðŸ”“ Unfreezing backbone\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 [TRAIN]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [00:44<00:00,  3.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/15\n",
      "Train Acc: 95.44%\n",
      "Val   Acc: 99.42%\n",
      "LR: 6.55e-05\n",
      "\n",
      "âœ” Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 [TRAIN]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [00:44<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/15\n",
      "Train Acc: 99.06%\n",
      "Val   Acc: 100.00%\n",
      "LR: 5.52e-05\n",
      "\n",
      "âœ” Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 [TRAIN]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [00:45<00:00,  3.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/15\n",
      "Train Acc: 99.93%\n",
      "Val   Acc: 99.71%\n",
      "LR: 4.48e-05\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 [TRAIN]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [00:44<00:00,  3.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/15\n",
      "Train Acc: 99.78%\n",
      "Val   Acc: 99.71%\n",
      "LR: 3.45e-05\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 [TRAIN]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [00:45<00:00,  3.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/15\n",
      "Train Acc: 99.78%\n",
      "Val   Acc: 99.71%\n",
      "LR: 2.50e-05\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11 [TRAIN]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [00:44<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/15\n",
      "Train Acc: 100.00%\n",
      "Val   Acc: 100.00%\n",
      "LR: 1.65e-05\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 [TRAIN]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [00:44<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/15\n",
      "Train Acc: 99.86%\n",
      "Val   Acc: 100.00%\n",
      "LR: 9.55e-06\n",
      "\n",
      "âš  Early stopping\n",
      "âœ… Training complete\n"
     ]
    }
   ],
   "source": [
    "# ---------------- DATA AUGMENTATION ----------------\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.6, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.8),\n",
    "    transforms.RandomRotation(25),\n",
    "    transforms.ColorJitter(0.4, 0.4, 0.4, 0.2),\n",
    "    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225]),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.05, 0.25)),\n",
    "])\n",
    "\n",
    "val_tf = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# ---------------- DATASETS ----------------\n",
    "full_dataset = datasets.ImageFolder(DATASET_DIR, transform=train_tf)\n",
    "num_val = int(len(full_dataset) * VALID_SPLIT)\n",
    "num_train = len(full_dataset) - num_val\n",
    "train_dataset, val_dataset = random_split(full_dataset, [num_train, num_val])\n",
    "val_dataset.dataset.transform = val_tf\n",
    "\n",
    "# ---- Class balancing ----\n",
    "labels = [full_dataset.targets[i] for i in train_dataset.indices]\n",
    "class_counts = np.bincount(labels)\n",
    "weights = 1.0 / class_counts\n",
    "sample_weights = [weights[y] for y in labels]\n",
    "\n",
    "sampler = WeightedRandomSampler(\n",
    "    sample_weights,\n",
    "    num_samples=len(sample_weights),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                          sampler=sampler, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n",
    "                        shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"Classes:\", full_dataset.classes)\n",
    "\n",
    "# ---------------- MODEL ----------------\n",
    "model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# ðŸ”’ SINGLE-LAYER FC â€” EXPORT SAFE\n",
    "model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "# ---- Freeze backbone ----\n",
    "for name, param in model.named_parameters():\n",
    "    if not name.startswith(\"fc\"):\n",
    "        param.requires_grad = False\n",
    "\n",
    "# ---------------- OPTIMIZATION ----------------\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LR,\n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=EPOCHS\n",
    ")\n",
    "\n",
    "scaler = amp.GradScaler()\n",
    "\n",
    "# ---------------- TRAINING ----------------\n",
    "best_val_acc = 0.0\n",
    "patience = 5\n",
    "no_improve = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    if epoch == FREEZE_EPOCHS:\n",
    "        print(\"ðŸ”“ Unfreezing backbone\")\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=LR,\n",
    "            weight_decay=WEIGHT_DECAY\n",
    "        )\n",
    "\n",
    "    # ---- TRAIN ----\n",
    "    model.train()\n",
    "    train_loss, correct, total = 0, 0, 0\n",
    "\n",
    "    for imgs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1} [TRAIN]\"):\n",
    "        imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with amp.autocast(device_type=DEVICE):\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss += loss.item() * imgs.size(0)\n",
    "        preds = outputs.argmax(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_acc = 100 * correct / total\n",
    "\n",
    "    # ---- VALIDATION ----\n",
    "    model.eval()\n",
    "    val_loss, correct, total = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "            with amp.autocast(device_type=DEVICE):\n",
    "                outputs = model(imgs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item() * imgs.size(0)\n",
    "            preds = outputs.argmax(1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    val_acc = 100 * correct / total\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"\"\"\n",
    "Epoch {epoch+1}/{EPOCHS}\n",
    "Train Acc: {train_acc:.2f}%\n",
    "Val   Acc: {val_acc:.2f}%\n",
    "LR: {scheduler.get_last_lr()[0]:.2e}\n",
    "\"\"\")\n",
    "\n",
    "    # ---- Checkpoint ----\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"cats_resnet50_best.pth\")\n",
    "        print(\"âœ” Saved best model\")\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            print(\"âš  Early stopping\")\n",
    "            break\n",
    "\n",
    "print(\"âœ… Training complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def export_resnet50_onnx(weights_path: str, output_path: str):\n",
    "    \"\"\"\n",
    "    Export ResNet50 to ONNX compatible with OpenCV DNN on Raspberry Pi\n",
    "    Assumes training used: model.fc = nn.Linear(...)\n",
    "    \"\"\"\n",
    "    NUM_CLASSES = 2\n",
    "    IMG_SIZE = 224\n",
    "\n",
    "    print(\"ðŸ”„ Building ResNet50 architecture (training-compatible)...\")\n",
    "\n",
    "    # ---- EXACT SAME ARCHITECTURE AS TRAINING ----\n",
    "    model = models.resnet50(weights=None)\n",
    "    model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "\n",
    "    print(\"ðŸ”„ Loading trained weights...\")\n",
    "    state = torch.load(weights_path, map_location=\"cpu\")\n",
    "    model.load_state_dict(state, strict=True)\n",
    "    model.eval()\n",
    "\n",
    "    print(\"ðŸ”„ Creating dummy input...\")\n",
    "    dummy_input = torch.randn(1, 3, IMG_SIZE, IMG_SIZE)\n",
    "\n",
    "    print(\"ðŸ”„ Exporting to ONNX (OpenCV-safe)...\")\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy_input,\n",
    "        output_path,\n",
    "        input_names=[\"image\"],\n",
    "        output_names=[\"logits\"],   # raw logits (1, 2)\n",
    "        opset_version=11,          # safest for OpenCV DNN\n",
    "        do_constant_folding=True,\n",
    "        dynamic_axes=None          # STATIC SHAPES (critical)\n",
    "    )\n",
    "\n",
    "    print(f\"âœ… Exported ONNX: {output_path}\")\n",
    "\n",
    "    # ---- Validate ----\n",
    "    onnx_model = onnx.load(output_path)\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "    print(\"âœ… ONNX model is valid\")\n",
    "\n",
    "    # ---- Repack into single file ----\n",
    "    single_file = output_path.replace(\".onnx\", \"_single.onnx\")\n",
    "    onnx.save_model(\n",
    "        onnx_model,\n",
    "        single_file,\n",
    "        save_as_external_data=False,\n",
    "        all_tensors_to_one_file=True\n",
    "    )\n",
    "\n",
    "    size_mb = Path(single_file).stat().st_size / (1024 * 1024)\n",
    "    print(f\"âœ” Single-file ONNX: {single_file} ({size_mb:.1f} MB)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Building ResNet50 architecture (training-compatible)...\n",
      "ðŸ”„ Loading trained weights...\n",
      "ðŸ”„ Creating dummy input...\n",
      "ðŸ”„ Exporting to ONNX (OpenCV-safe)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0210 14:10:15.090276 8253 site-packages/torch/onnx/_internal/exporter/_compat.py:125] Setting ONNX exporter to use operator set version 18 because the requested opset_version 11 is a lower version than we have implementations for. Automatic version conversion will be performed, which may not be successful at converting to the requested version. If version conversion is unsuccessful, the opset version of the exported model will be kept at 18. Please consider setting opset_version >=18 to leverage latest ONNX features\n",
      "W0210 14:10:15.683768 8253 site-packages/torch/onnx/_internal/exporter/_schemas.py:455] Missing annotation for parameter 'input' from (input, boxes, output_size: 'Sequence[int]', spatial_scale: 'float' = 1.0, sampling_ratio: 'int' = -1, aligned: 'bool' = False). Treating as an Input.\n",
      "W0210 14:10:15.684347 8253 site-packages/torch/onnx/_internal/exporter/_schemas.py:455] Missing annotation for parameter 'boxes' from (input, boxes, output_size: 'Sequence[int]', spatial_scale: 'float' = 1.0, sampling_ratio: 'int' = -1, aligned: 'bool' = False). Treating as an Input.\n",
      "W0210 14:10:15.684933 8253 site-packages/torch/onnx/_internal/exporter/_schemas.py:455] Missing annotation for parameter 'input' from (input, boxes, output_size: 'Sequence[int]', spatial_scale: 'float' = 1.0). Treating as an Input.\n",
      "W0210 14:10:15.685459 8253 site-packages/torch/onnx/_internal/exporter/_schemas.py:455] Missing annotation for parameter 'boxes' from (input, boxes, output_size: 'Sequence[int]', spatial_scale: 'float' = 1.0). Treating as an Input.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `ResNet([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `ResNet([...]` with `torch.export.export(..., strict=False)`... âœ…\n",
      "[torch.onnx] Run decomposition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copyreg.py:101: FutureWarning: `isinstance(treespec, LeafSpec)` is deprecated, use `isinstance(treespec, TreeSpec) and treespec.is_leaf()` instead.\n",
      "  return cls.__new__(cls, *args)\n",
      "The model version conversion is not supported by the onnxscript version converter and fallback is enabled. The model will be converted using the onnx C API (target version: 11).\n",
      "Failed to convert the model to the target version 11 using the ONNX C API. The model was not modified\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/onnxscript/version_converter/__init__.py\", line 127, in call\n",
      "    converted_proto = _c_api_utils.call_onnx_api(\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/onnxscript/version_converter/_c_api_utils.py\", line 65, in call_onnx_api\n",
      "    result = func(proto)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/onnxscript/version_converter/__init__.py\", line 122, in _partial_convert_version\n",
      "    return onnx.version_converter.convert_version(\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/onnx/version_converter.py\", line 39, in convert_version\n",
      "    converted_model_str = C.convert_version(model_str, target_version)\n",
      "RuntimeError: /Users/runner/work/onnx/onnx/onnx/version_converter/adapters/axes_input_to_attribute.h:65: adapt: Assertion `node->hasAttribute(kaxes)` failed: No initializer or constant input to node found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Run decomposition... âœ…\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... âœ…\n",
      "Applied 106 of general pattern rewrite rules.\n",
      "âœ… Exported ONNX: cats_resnet50.onnx\n",
      "âœ… ONNX model is valid\n",
      "âœ” Single-file ONNX: cats_resnet50_single.onnx (89.8 MB)\n"
     ]
    }
   ],
   "source": [
    "export_resnet50_onnx(\n",
    "    weights_path=\"cats_resnet50_best.pth\",\n",
    "    output_path=\"cats_resnet50.onnx\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
